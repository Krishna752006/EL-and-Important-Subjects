{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd72887a-ad15-48c6-a9f5-5d4bfbfe7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afe07f3-c7db-464b-a2eb-ad800360783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Machine learning is fascinating.\n",
    "It allows computers to learn from data.\n",
    "The more data, the better the learning.\n",
    "Deep learning is a subset of machine learning.\n",
    "Neural networks are at the core of deep learning.\n",
    "Artificial intelligence is evolving rapidly.\n",
    "Data science combines domain expertise with programming skills.\n",
    "Big data plays a  CRUCIAL role in  MODERN analytics.\n",
    "Natural language processing is a key part of AI.\n",
    "Predictive modeling helps in forecasting future trends.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c256a3f0-55dd-494f-ae29-241615244449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ['Machine', 'learning', 'is', 'fascinating', 'It', 'allows', 'computers', 'to', 'learn', 'from', 'data', 'The', 'more', 'data', 'the', 'better', 'the', 'learning', 'Deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning', 'Neural', 'networks', 'are', 'at', 'the', 'core', 'of', 'deep', 'learning', 'Artificial', 'intelligence', 'is', 'evolving', 'rapidly', 'Data', 'science', 'combines', 'domain', 'expertise', 'with', 'programming', 'skills', 'Big', 'data', 'plays', 'a', 'CRUCIAL', 'role', 'in', 'MODERN', 'analytics', 'Natural', 'language', 'processing', 'is', 'a', 'key', 'part', 'of', 'AI', 'Predictive', 'modeling', 'helps', 'in', 'forecasting', 'future', 'trends']\n",
      "Lower Tokens ['machine', 'learning', 'is', 'fascinating', 'it', 'allows', 'computers', 'to', 'learn', 'from', 'data', 'the', 'more', 'data', 'the', 'better', 'the', 'learning', 'deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning', 'neural', 'networks', 'are', 'at', 'the', 'core', 'of', 'deep', 'learning', 'artificial', 'intelligence', 'is', 'evolving', 'rapidly', 'data', 'science', 'combines', 'domain', 'expertise', 'with', 'programming', 'skills', 'big', 'data', 'plays', 'a', 'crucial', 'role', 'in', 'modern', 'analytics', 'natural', 'language', 'processing', 'is', 'a', 'key', 'part', 'of', 'ai', 'predictive', 'modeling', 'helps', 'in', 'forecasting', 'future', 'trends']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    tokens=[]\n",
    "    word=\"\"\n",
    "    for char in text:\n",
    "        if char in string.whitespace or char in string.punctuation:\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "                word=\"\"\n",
    "        else:\n",
    "            word+=char\n",
    "    if word:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "tokens=tokenize(paragraph)\n",
    "lower_tokens=[word.lower() for word in tokens]\n",
    "print(\"Tokens\",tokens)\n",
    "print(\"Lower Tokens\",lower_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d0a4be-db17-4d0c-8312-400be04834f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized tokens ['machine', 'learn', 'is', 'fascinating', 'it', 'allows', 'computer', 'to', 'learn', 'from', 'datum', 'the', 'more', 'datum', 'the', 'better', 'the', 'learn', 'deep', 'learn', 'is', 'a', 'subset', 'of', 'machine', 'learn', 'neural', 'network', 'are', 'at', 'the', 'core', 'of', 'deep', 'learn', 'artificial', 'intelligence', 'is', 'evolving', 'rapidly', 'datum', 'science', 'combines', 'domain', 'expertise', 'with', 'programming', 'skills', 'big', 'datum', 'plays', 'a', 'crucial', 'role', 'in', 'modern', 'analytics', 'natural', 'language', 'processing', 'is', 'a', 'key', 'part', 'of', 'ai', 'predictive', 'modeling', 'helps', 'in', 'forecasting', 'future', 'trends']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(token):\n",
    "    lemmas={\"learning\":\"learn\",\n",
    "            \"computers\":\"computer\",\n",
    "            \"data\":\"datum\",\n",
    "            \"networks\":\"network\"}\n",
    "    \n",
    "    return lemmas.get(token,token)\n",
    "\n",
    "lemmatized_tokens=[lemmatize(token) for token in lower_tokens]\n",
    "print(\"lemmatized tokens\",lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd72edd8-07da-45b8-b09b-cee52247da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed_tokens ['machine', 'learn', 'i', 'fascinat', 'it', 'allow', 'computer', 'to', 'learn', 'from', 'datum', 'the', 'more', 'datum', 'the', 'better', 'the', 'learn', 'deep', 'learn', 'i', 'a', 'subset', 'of', 'machine', 'learn', 'neural', 'network', 'are', 'at', 'the', 'core', 'of', 'deep', 'learn', 'artificial', 'intelligence', 'i', 'evolv', 'rapidly', 'datum', 'science', 'combine', 'domain', 'expertise', 'with', 'programm', 'skill', 'big', 'datum', 'play', 'a', 'crucial', 'role', 'in', 'modern', 'analytic', 'natural', 'language', 'process', 'i', 'a', 'key', 'part', 'of', 'ai', 'predictive', 'model', 'help', 'in', 'forecast', 'future', 'trend']\n"
     ]
    }
   ],
   "source": [
    "def stem(token):\n",
    "    suffixes=[\"ing\",\"ed\",\"s\"]\n",
    "    for suffix in suffixes:\n",
    "        if token.endswith(suffix):\n",
    "            return token[:-len(suffix)]\n",
    "    return token\n",
    "\n",
    "stemmed_tokens=[stem(token) for token in lemmatized_tokens]\n",
    "print(\"stemmed_tokens\",stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403c88b9-2474-44e3-b694-aa554b018bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered tokens ['machine', 'learn', 'i', 'fascinat', 'it', 'allow', 'computer', 'learn', 'datum', 'more', 'datum', 'better', 'learn', 'deep', 'learn', 'i', 'subset', 'machine', 'learn', 'neural', 'network', 'core', 'deep', 'learn', 'artificial', 'intelligence', 'i', 'evolv', 'rapidly', 'datum', 'science', 'combine', 'domain', 'expertise', 'with', 'programm', 'skill', 'big', 'datum', 'play', 'crucial', 'role', 'in', 'modern', 'analytic', 'natural', 'language', 'process', 'i', 'key', 'part', 'ai', 'predictive', 'model', 'help', 'in', 'forecast', 'future', 'trend']\n"
     ]
    }
   ],
   "source": [
    "stop_words=[\"is\",\"to\",\"the\",\"from\",\"and\",\"are\",\"at\",\"of\",\"a\"]\n",
    "filtered_tokens=[token for token in stemmed_tokens if token not in stop_words]\n",
    "print(\"filtered tokens\",filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b02d8e9-0a21-4395-8fb9-8630a04e305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vector(tokens,vector_size=50):\n",
    "    vocab=set(tokens)\n",
    "    word_vectors={word:torch.rand(vector_size) for word in vocab}\n",
    "    \n",
    "    return word_vectors\n",
    "word_vectors=create_word_vector(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af403fb-b408-419e-a0a0-16c86f98cb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to index {'machine': 17, 'learn': 23, 'i': 48, 'fascinat': 3, 'it': 4, 'allow': 5, 'computer': 6, 'datum': 38, 'more': 9, 'better': 11, 'deep': 22, 'subset': 16, 'neural': 19, 'network': 20, 'core': 21, 'artificial': 24, 'intelligence': 25, 'evolv': 27, 'rapidly': 28, 'science': 30, 'combine': 31, 'domain': 32, 'expertise': 33, 'with': 34, 'programm': 35, 'skill': 36, 'big': 37, 'play': 39, 'crucial': 40, 'role': 41, 'in': 55, 'modern': 43, 'analytic': 44, 'natural': 45, 'language': 46, 'process': 47, 'key': 49, 'part': 50, 'ai': 51, 'predictive': 52, 'model': 53, 'help': 54, 'forecast': 56, 'future': 57, 'trend': 58}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:idx for idx ,word in enumerate(filtered_tokens)}\n",
    "print(\"word to index\",word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfffff-dd91-4b9c-9e98-cf87196d46d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
