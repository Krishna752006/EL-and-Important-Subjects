{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520abf51-b4ab-466e-8ad6-e55c36580c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for 'I love AI .': [1, 2, 3, 4]\n",
      "Tokens for 'J' adore l'IA .': [1, 2, 3, 4]\n",
      "\n",
      "=== Encoder Input Tokens ===\n",
      "tensor([[1, 2, 3, 4]])\n",
      "\n",
      "=== Token Embeddings ===\n",
      "tensor([[[ 0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168,\n",
      "          -0.2473],\n",
      "         [-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,\n",
      "           1.8530],\n",
      "         [-0.2159, -0.7425,  0.5627,  0.2596, -0.1740, -0.6787,  0.9383,\n",
      "           0.4889],\n",
      "         [ 1.2032,  0.0845, -1.2001, -0.0048, -0.5181, -0.3067, -1.5810,\n",
      "           1.7066]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional Encoding Values:\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n",
      "\n",
      "Embeddings After Adding Positional Encoding:\n",
      "tensor([[[ 0.3223, -0.2633,  0.3500,  1.3081,  0.1198,  2.2377,  1.1168,\n",
      "           0.7527],\n",
      "         [-0.5112, -1.1556,  0.6665,  1.7885,  0.6088, -0.5551, -0.3404,\n",
      "           2.8530],\n",
      "         [ 0.6934, -1.1587,  0.7614,  1.2397, -0.1540,  0.3211,  0.9403,\n",
      "           1.4889],\n",
      "         [ 1.3443, -0.9055, -0.9046,  0.9506, -0.4881,  0.6928, -1.5780,\n",
      "           2.7066]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "=== Encoder Layer Input ===\n",
      "tensor([[[ 0.3223, -0.2633,  0.3500,  1.3081,  0.1198,  2.2377,  1.1168,\n",
      "           0.7527],\n",
      "         [-0.5112, -1.1556,  0.6665,  1.7885,  0.6088, -0.5551, -0.3404,\n",
      "           2.8530],\n",
      "         [ 0.6934, -1.1587,  0.7614,  1.2397, -0.1540,  0.3211,  0.9403,\n",
      "           1.4889],\n",
      "         [ 1.3443, -0.9055, -0.9046,  0.9506, -0.4881,  0.6928, -1.5780,\n",
      "           2.7066]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Query Matrix (Q):\n",
      "tensor([[[[ 0.0913,  0.5915, -0.4988, -0.7010],\n",
      "          [ 0.3263, -0.2028, -0.2477, -0.8475],\n",
      "          [ 0.0722, -0.0546, -0.2029, -0.9827],\n",
      "          [ 1.1636,  0.5956, -1.1301, -1.5583]],\n",
      "\n",
      "         [[-0.1982, -0.2012, -0.5326, -0.4854],\n",
      "          [-0.2150, -1.0644,  0.0323, -0.6455],\n",
      "          [-0.3489, -0.8874, -0.6592, -0.6862],\n",
      "          [-0.0146,  0.0219,  0.2887, -1.0550]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Key Matrix (K):\n",
      "tensor([[[[ 0.0467,  1.2442, -1.4476, -1.4320],\n",
      "          [-0.0862,  0.3878, -0.4321, -0.9602],\n",
      "          [ 0.2505,  0.6892, -0.9101, -1.1527],\n",
      "          [-0.4113, -0.4012, -1.1319, -1.5330]],\n",
      "\n",
      "         [[-0.7956,  0.0815, -0.2619,  0.7765],\n",
      "          [-0.4500,  1.1305, -0.0702, -0.1176],\n",
      "          [-0.7355,  0.0108,  0.1597,  0.5390],\n",
      "          [ 0.4062,  0.3381, -0.8510,  0.6006]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Value Matrix (V):\n",
      "tensor([[[[ 0.2737, -0.3252,  0.5152, -0.1674],\n",
      "          [-0.1233, -0.1605, -0.2907,  0.4988],\n",
      "          [-0.1943, -0.1379,  0.0370, -0.1878],\n",
      "          [ 1.2323, -0.0024, -0.7535,  0.4712]],\n",
      "\n",
      "         [[-0.6246, -0.6531, -0.6830,  0.1546],\n",
      "          [ 0.2295, -0.9654, -0.2071, -0.2330],\n",
      "          [-0.2488, -0.5490, -0.6483, -0.3392],\n",
      "          [-0.0775, -1.4040, -0.1304, -0.1732]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Raw Attention Scores (before softmax):\n",
      "tensor([[[[ 1.2331,  0.5551,  0.8463,  0.6822],\n",
      "          [ 0.6675,  0.4070,  0.5721,  0.7633],\n",
      "          [ 0.8182,  0.5019,  0.6489,  0.8641],\n",
      "          [ 2.3314,  1.0576,  1.7634,  1.4752]],\n",
      "\n",
      "         [[-0.0480, -0.0219, -0.1015,  0.0066],\n",
      "          [-0.2127, -0.5164, -0.0980, -0.4312],\n",
      "          [-0.0775, -0.3596, -0.1140, -0.1464],\n",
      "          [-0.4407,  0.0676, -0.2558, -0.4389]]]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Attention Weights (after softmax):\n",
      "tensor([[[[0.3619, 0.1837, 0.2458, 0.2086],\n",
      "          [0.2645, 0.2039, 0.2405, 0.2911],\n",
      "          [0.2762, 0.2013, 0.2332, 0.2892],\n",
      "          [0.4403, 0.1232, 0.2495, 0.1870]],\n",
      "\n",
      "         [[0.2481, 0.2547, 0.2352, 0.2620],\n",
      "          [0.2730, 0.2015, 0.3061, 0.2194],\n",
      "          [0.2738, 0.2065, 0.2640, 0.2556],\n",
      "          [0.2055, 0.3415, 0.2472, 0.2058]]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention Output (before final FC layer):\n",
      "tensor([[[ 0.2857, -0.1815, -0.0151,  0.0832, -0.1754, -0.9049, -0.4088,\n",
      "          -0.1462],\n",
      "         [ 0.3593, -0.1526, -0.1335,  0.1494, -0.2175, -0.8489, -0.4553,\n",
      "          -0.1466],\n",
      "         [ 0.3619, -0.1550, -0.1255,  0.1467, -0.2092, -0.8820, -0.4343,\n",
      "          -0.1396],\n",
      "         [ 0.2873, -0.1978,  0.0593,  0.0290, -0.1274, -0.8886, -0.3982,\n",
      "          -0.1673]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Output from Multi-Head Attention (after FC):\n",
      "tensor([[[ 0.2894,  0.1853,  0.3535,  0.4151,  0.1403, -0.0826, -0.1259,\n",
      "           0.0704],\n",
      "         [ 0.2456,  0.2512,  0.3791,  0.4197,  0.1459, -0.0764, -0.0898,\n",
      "           0.1011],\n",
      "         [ 0.2539,  0.2377,  0.3749,  0.4156,  0.1429, -0.0738, -0.0960,\n",
      "           0.0918],\n",
      "         [ 0.2985,  0.1567,  0.3401,  0.4261,  0.1449, -0.0909, -0.1306,\n",
      "           0.0832]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "=== After Self-Attention and Norm (Encoder) ===\n",
      "tensor([[[-0.4189, -1.4257, -0.2850,  1.2035, -0.9321,  1.8338,  0.1346,\n",
      "          -0.1103],\n",
      "         [-0.6492, -1.1331,  0.3441,  1.2248,  0.1238, -0.9264, -0.7738,\n",
      "           1.7899],\n",
      "         [ 0.3225, -1.9735,  0.5548,  1.1926, -0.8553, -0.5378,  0.1959,\n",
      "           1.1009],\n",
      "         [ 0.9068, -0.8116, -0.6792,  0.7156, -0.5202,  0.1589, -1.5012,\n",
      "           1.7309]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Input to FeedForward Network:\n",
      "tensor([[[-0.4189, -1.4257, -0.2850,  1.2035, -0.9321,  1.8338,  0.1346,\n",
      "          -0.1103],\n",
      "         [-0.6492, -1.1331,  0.3441,  1.2248,  0.1238, -0.9264, -0.7738,\n",
      "           1.7899],\n",
      "         [ 0.3225, -1.9735,  0.5548,  1.1926, -0.8553, -0.5378,  0.1959,\n",
      "           1.1009],\n",
      "         [ 0.9068, -0.8116, -0.6792,  0.7156, -0.5202,  0.1589, -1.5012,\n",
      "           1.7309]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "After first linear layer (fc1):\n",
      "tensor([[[-0.1336,  0.2462,  1.2741,  0.7872, -0.9029, -0.0608,  0.5490,\n",
      "          -0.4681, -0.6866,  0.0562,  0.3810, -1.0267,  2.0538,  0.3734,\n",
      "          -0.0513, -0.2711,  0.2354,  1.2497, -0.0431,  1.5076,  0.4016,\n",
      "           0.9388, -0.2645, -0.0983,  0.7074,  0.1623,  0.3799,  1.4529,\n",
      "          -0.8970,  0.3719,  0.8976,  0.7914],\n",
      "         [ 0.2435,  0.1172, -0.5164,  0.8096,  0.3189, -0.6606,  1.4523,\n",
      "           0.4857, -0.4471,  1.0148,  0.1676, -1.1839,  1.1523,  1.1156,\n",
      "          -0.7703,  0.3178, -0.0215,  0.4606,  0.4011,  0.9262,  0.4211,\n",
      "           0.4599, -0.4418,  0.8310, -0.0811, -1.3485,  0.2436,  0.1882,\n",
      "           0.3983,  1.5632, -0.1058,  0.1387],\n",
      "         [-0.2547, -0.1617, -0.0907,  0.5686, -0.0064, -0.5278,  0.7120,\n",
      "           0.5349, -1.2291,  0.7729,  0.4678, -0.7462,  1.3213,  0.7506,\n",
      "          -0.3976,  0.4220,  0.2951,  0.5163,  0.5327,  1.2997,  0.7899,\n",
      "           0.8942,  0.2832,  0.9282,  0.2477, -1.4258,  1.0338,  0.3189,\n",
      "           0.3615,  1.3557,  0.0238,  0.1104],\n",
      "         [ 0.0684,  0.3610, -0.8030,  0.7710,  0.0341, -0.4253,  1.1631,\n",
      "           0.2049, -0.3385,  0.3701,  0.9454, -0.8550,  1.0965,  0.7611,\n",
      "          -0.3099,  0.5489,  0.8409,  0.3678, -0.1568,  0.2592,  0.8944,\n",
      "           0.1081, -0.5898,  0.2931,  0.0731, -0.2188,  0.4577,  0.1884,\n",
      "           0.2919,  0.9247, -0.1726, -0.2008]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "After ReLU Activation:\n",
      "tensor([[[0.0000, 0.2462, 1.2741, 0.7872, 0.0000, 0.0000, 0.5490, 0.0000,\n",
      "          0.0000, 0.0562, 0.3810, 0.0000, 2.0538, 0.3734, 0.0000, 0.0000,\n",
      "          0.2354, 1.2497, 0.0000, 1.5076, 0.4016, 0.9388, 0.0000, 0.0000,\n",
      "          0.7074, 0.1623, 0.3799, 1.4529, 0.0000, 0.3719, 0.8976, 0.7914],\n",
      "         [0.2435, 0.1172, 0.0000, 0.8096, 0.3189, 0.0000, 1.4523, 0.4857,\n",
      "          0.0000, 1.0148, 0.1676, 0.0000, 1.1523, 1.1156, 0.0000, 0.3178,\n",
      "          0.0000, 0.4606, 0.4011, 0.9262, 0.4211, 0.4599, 0.0000, 0.8310,\n",
      "          0.0000, 0.0000, 0.2436, 0.1882, 0.3983, 1.5632, 0.0000, 0.1387],\n",
      "         [0.0000, 0.0000, 0.0000, 0.5686, 0.0000, 0.0000, 0.7120, 0.5349,\n",
      "          0.0000, 0.7729, 0.4678, 0.0000, 1.3213, 0.7506, 0.0000, 0.4220,\n",
      "          0.2951, 0.5163, 0.5327, 1.2997, 0.7899, 0.8942, 0.2832, 0.9282,\n",
      "          0.2477, 0.0000, 1.0338, 0.3189, 0.3615, 1.3557, 0.0238, 0.1104],\n",
      "         [0.0684, 0.3610, 0.0000, 0.7710, 0.0341, 0.0000, 1.1631, 0.2049,\n",
      "          0.0000, 0.3701, 0.9454, 0.0000, 1.0965, 0.7611, 0.0000, 0.5489,\n",
      "          0.8409, 0.3678, 0.0000, 0.2592, 0.8944, 0.1081, 0.0000, 0.2931,\n",
      "          0.0731, 0.0000, 0.4577, 0.1884, 0.2919, 0.9247, 0.0000, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "\n",
      "After second linear layer (fc2):\n",
      "tensor([[[-0.2991,  0.0663, -0.5561, -0.0593, -0.3741,  0.0484,  0.2162,\n",
      "           0.4083],\n",
      "         [-0.2543,  0.2154, -0.2286,  0.0491,  0.2431,  0.0490, -0.0442,\n",
      "           0.2684],\n",
      "         [-0.3306,  0.4232, -0.1302,  0.0959,  0.1072,  0.0170,  0.0084,\n",
      "           0.1848],\n",
      "         [-0.1303,  0.4836, -0.4902,  0.1578,  0.2569,  0.3000, -0.1301,\n",
      "          -0.0057]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "=== After FeedForward and Norm (Encoder) ===\n",
      "tensor([[[-0.5865, -1.1660, -0.6977,  1.0957, -1.1179,  1.7624,  0.3789,\n",
      "           0.3312],\n",
      "         [-0.8823, -0.8957,  0.0734,  1.1599,  0.3092, -0.8579, -0.8022,\n",
      "           1.8956],\n",
      "         [-0.0600, -1.7386,  0.4110,  1.3514, -0.8654, -0.6180,  0.1713,\n",
      "           1.3483],\n",
      "         [ 0.6903, -0.3667, -1.1721,  0.7830, -0.3048,  0.3863, -1.6142,\n",
      "           1.5982]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "=== Final Encoder Output ===\n",
      "tensor([[[-0.5865, -1.1660, -0.6977,  1.0957, -1.1179,  1.7624,  0.3789,\n",
      "           0.3312],\n",
      "         [-0.8823, -0.8957,  0.0734,  1.1599,  0.3092, -0.8579, -0.8022,\n",
      "           1.8956],\n",
      "         [-0.0600, -1.7386,  0.4110,  1.3514, -0.8654, -0.6180,  0.1713,\n",
      "           1.3483],\n",
      "         [ 0.6903, -0.3667, -1.1721,  0.7830, -0.3048,  0.3863, -1.6142,\n",
      "           1.5982]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "=== Decoder Input Tokens ===\n",
      "tensor([[1, 2, 3, 4]])\n",
      "\n",
      "=== Decoder Token Embeddings ===\n",
      "tensor([[[-0.2746,  0.4432,  0.9591,  1.3320,  0.7733,  3.0567, -2.4878,\n",
      "          -1.6962],\n",
      "         [-0.0510,  1.3926,  2.2975,  0.5273,  0.1514,  0.5188, -0.5896,\n",
      "           1.0007],\n",
      "         [-0.6225, -0.3563,  1.9632,  0.2342, -0.4766, -0.1469,  0.2829,\n",
      "          -0.4333],\n",
      "         [ 1.7847,  0.1933,  0.3247,  0.0425, -1.2903,  0.5976,  0.7983,\n",
      "           1.2603]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional Encoding Values:\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n",
      "\n",
      "Embeddings After Adding Positional Encoding:\n",
      "tensor([[[-0.2746,  1.4432,  0.9591,  2.3320,  0.7733,  4.0567, -2.4878,\n",
      "          -0.6962],\n",
      "         [ 0.7905,  1.9329,  2.3974,  1.5223,  0.1614,  1.5188, -0.5886,\n",
      "           2.0007],\n",
      "         [ 0.2868, -0.7724,  2.1619,  1.2143, -0.4566,  0.8529,  0.2849,\n",
      "           0.5667],\n",
      "         [ 1.9258, -0.7967,  0.6202,  0.9978, -1.2603,  1.5971,  0.8013,\n",
      "           2.2603]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "=== Decoder Layer Input ===\n",
      "tensor([[[-0.2746,  1.4432,  0.9591,  2.3320,  0.7733,  4.0567, -2.4878,\n",
      "          -0.6962],\n",
      "         [ 0.7905,  1.9329,  2.3974,  1.5223,  0.1614,  1.5188, -0.5886,\n",
      "           2.0007],\n",
      "         [ 0.2868, -0.7724,  2.1619,  1.2143, -0.4566,  0.8529,  0.2849,\n",
      "           0.5667],\n",
      "         [ 1.9258, -0.7967,  0.6202,  0.9978, -1.2603,  1.5971,  0.8013,\n",
      "           2.2603]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Query Matrix (Q):\n",
      "tensor([[[[-0.4515, -0.9495,  2.0963, -0.9340],\n",
      "          [-0.0114, -0.4111,  1.3001, -0.6784],\n",
      "          [ 0.9730,  0.2698, -0.0496, -0.9505],\n",
      "          [ 0.0472, -0.4778, -0.2757, -0.6218]],\n",
      "\n",
      "         [[ 0.0102, -0.4942,  0.2686, -0.6906],\n",
      "          [-0.0572, -0.0835, -1.2470,  0.2029],\n",
      "          [ 0.1771,  0.2617, -0.3574,  0.2224],\n",
      "          [ 1.4139, -0.2528, -0.6501, -0.0251]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Key Matrix (K):\n",
      "tensor([[[[-0.1920, -0.2142, -0.5263,  2.2328],\n",
      "          [-0.6676,  0.0297,  0.0503,  0.5421],\n",
      "          [ 0.3341, -0.0131, -0.7642, -0.3636],\n",
      "          [ 1.0943,  0.3272, -0.6775, -0.9939]],\n",
      "\n",
      "         [[ 0.4588,  0.0947, -2.3442,  0.2990],\n",
      "          [ 1.9046, -0.6435, -0.4221,  0.0245],\n",
      "          [ 0.9152,  0.0223,  0.5900,  0.5626],\n",
      "          [ 1.9887, -0.0110,  0.8447,  0.7015]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Value Matrix (V):\n",
      "tensor([[[[-2.0833, -0.0238, -1.2195, -0.0432],\n",
      "          [-0.7785, -1.6660, -1.8367, -1.3839],\n",
      "          [ 0.0432, -1.5569, -1.8377, -0.0464],\n",
      "          [ 0.6354, -2.2067, -2.1023, -0.0750]],\n",
      "\n",
      "         [[ 1.6404,  0.4154, -2.4350,  0.8295],\n",
      "          [-0.3912, -0.1814, -1.9069,  0.7522],\n",
      "          [ 0.1831, -0.1258, -0.2494,  0.5443],\n",
      "          [ 0.6354, -0.7872, -0.8518,  0.2684]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Raw Attention Scores (before softmax):\n",
      "tensor([[[[-1.4493, -0.0638, -0.7003, -0.6484],\n",
      "          [-1.0544, -0.1535, -0.3726, -0.1768],\n",
      "          [-1.1704, -0.5797,  0.3525,  1.0657],\n",
      "          [-0.5750, -0.1983,  0.2294,  0.3501]],\n",
      "\n",
      "         [[-0.4392,  0.1036, -0.1158, -0.1159],\n",
      "          [ 1.4749,  0.2381, -0.3379, -0.5119],\n",
      "          [ 0.5052,  0.1626,  0.0411,  0.1017],\n",
      "          [ 1.0707,  1.5647,  0.4454,  1.1239]]]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Attention Weights (after softmax):\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2889, 0.7111, 0.0000, 0.0000],\n",
      "          [0.1353, 0.2443, 0.6204, 0.0000],\n",
      "          [0.1386, 0.2020, 0.3098, 0.3496]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7750, 0.2250, 0.0000, 0.0000],\n",
      "          [0.4276, 0.3036, 0.2688, 0.0000],\n",
      "          [0.2365, 0.3876, 0.1265, 0.2494]]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention Output (before final FC layer):\n",
      "tensor([[[-2.0833, -0.0238, -1.2195, -0.0432,  1.6404,  0.4154, -2.4350,\n",
      "           0.8295],\n",
      "         [-1.1554, -1.1916, -1.6584, -0.9966,  1.1833,  0.2811, -2.3162,\n",
      "           0.8121],\n",
      "         [-0.4452, -1.3761, -1.7538, -0.3726,  0.6319,  0.0887, -1.6871,\n",
      "           0.7293],\n",
      "         [-0.2105, -1.5936, -1.8443, -0.3261,  0.4180, -0.1843, -1.5589,\n",
      "           0.6235]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Output from Multi-Head Attention (after FC):\n",
      "tensor([[[ 0.0495, -1.3309,  1.3097, -0.7711, -0.1819,  0.9756,  0.8189,\n",
      "          -0.1244],\n",
      "         [ 0.0728, -0.9242,  1.0963, -0.8012, -0.1575,  1.4181,  0.6332,\n",
      "          -0.4671],\n",
      "         [ 0.3111, -0.7130,  1.0288, -0.8745, -0.3543,  1.1336,  0.5817,\n",
      "          -0.3079],\n",
      "         [ 0.4528, -0.5694,  1.0509, -0.8953, -0.4078,  1.2304,  0.5211,\n",
      "          -0.3093]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "=== After Masked Self-Attention and Norm (Decoder) ===\n",
      "tensor([[[-0.5502, -0.3785,  0.7185,  0.3584, -0.1348,  2.1244, -1.2847,\n",
      "          -0.8531],\n",
      "         [-0.3872, -0.2655,  1.8155, -0.5064, -1.1069,  1.3492, -1.0729,\n",
      "           0.1741],\n",
      "         [-0.0145, -1.5246,  1.8648, -0.2017, -1.0358,  0.9920,  0.1802,\n",
      "          -0.2603],\n",
      "         [ 0.9303, -1.4295,  0.4845, -0.5041, -1.6198,  1.2131,  0.2647,\n",
      "           0.6608]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Query Matrix (Q):\n",
      "tensor([[[[ 0.1153, -0.7502,  0.0335,  0.2261],\n",
      "          [ 0.7346, -0.5131,  0.1179, -0.1609],\n",
      "          [ 0.1847,  0.0857,  1.2113, -0.4745],\n",
      "          [-0.2788, -0.0512,  1.0383, -0.4168]],\n",
      "\n",
      "         [[-0.3421, -0.1904,  1.1979, -0.4371],\n",
      "          [-0.5610, -0.0473,  1.0147,  0.4951],\n",
      "          [-0.3349, -0.2626,  1.1920,  0.7596],\n",
      "          [-0.4413, -0.4889,  0.4308,  0.8771]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Key Matrix (K):\n",
      "tensor([[[[ 0.5846,  0.5371, -0.1693,  0.7032],\n",
      "          [ 0.5942, -0.7331, -0.2736,  0.6069],\n",
      "          [ 1.4369, -0.3953, -0.6632,  1.0325],\n",
      "          [-0.1181, -0.0089, -0.0764,  0.7717]],\n",
      "\n",
      "         [[ 0.6260, -0.3880,  0.0508,  0.3777],\n",
      "          [ 0.7170, -0.3412, -0.7994,  0.1668],\n",
      "          [ 0.4478, -0.0694, -0.8198,  0.0248],\n",
      "          [ 0.6332,  0.0608, -0.0476,  0.1524]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Value Matrix (V):\n",
      "tensor([[[[ 0.1270, -1.5953,  0.8948,  0.6892],\n",
      "          [-0.3232, -1.0978,  0.1067, -0.1438],\n",
      "          [-0.2386, -1.1091,  0.4823,  0.2063],\n",
      "          [ 0.7526, -1.2593,  0.2056, -0.1317]],\n",
      "\n",
      "         [[ 0.1392, -0.4512, -0.4238, -0.2313],\n",
      "          [ 0.4828,  0.8147,  1.0541, -0.5181],\n",
      "          [ 0.7633,  0.4453,  0.7350, -0.7267],\n",
      "          [ 0.6879, -0.1437,  1.4224, -0.2881]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Raw Attention Scores (before softmax):\n",
      "tensor([[[[-0.0911,  0.3733,  0.3368,  0.0825],\n",
      "          [ 0.0104,  0.3414,  0.5070, -0.1077],\n",
      "          [-0.1924, -0.2863, -0.5309, -0.2407],\n",
      "          [-0.3297, -0.3326, -0.7497, -0.1838]],\n",
      "\n",
      "         [[-0.1222, -0.6055, -0.5665, -0.1759],\n",
      "          [-0.0471, -0.5573, -0.5338, -0.1655],\n",
      "          [ 0.1199, -0.4884, -0.5451, -0.0845],\n",
      "          [ 0.1333, -0.1739, -0.2476, -0.0980]]]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Attention Weights (after softmax):\n",
      "tensor([[[[0.1882, 0.2994, 0.2886, 0.2238],\n",
      "          [0.2031, 0.2828, 0.3337, 0.1805],\n",
      "          [0.2796, 0.2546, 0.1993, 0.2665],\n",
      "          [0.2624, 0.2616, 0.1724, 0.3036]],\n",
      "\n",
      "         [[0.3119, 0.1924, 0.2001, 0.2956],\n",
      "          [0.3222, 0.1935, 0.1981, 0.2863],\n",
      "          [0.3480, 0.1894, 0.1790, 0.2837],\n",
      "          [0.3113, 0.2290, 0.2127, 0.2470]]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention Output (before final FC layer):\n",
      "tensor([[[ 0.0267, -1.2308,  0.3855,  0.1167,  0.4924,  0.0626,  0.6382,\n",
      "          -0.4024],\n",
      "         [-0.0094, -1.2317,  0.4099,  0.1444,  0.4864,  0.0593,  0.6201,\n",
      "          -0.4012],\n",
      "         [ 0.1062, -1.2822,  0.4283,  0.1621,  0.4716,  0.0362,  0.5872,\n",
      "          -0.3904],\n",
      "         [ 0.1361, -1.2793,  0.4083,  0.1388,  0.4862,  0.1053,  0.6171,\n",
      "          -0.4164]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Output from Multi-Head Attention (after FC):\n",
      "tensor([[[ 0.2873,  0.8071, -0.6078, -0.1136,  0.1601,  1.0617, -0.2076,\n",
      "           0.2621],\n",
      "         [ 0.2941,  0.8179, -0.6364, -0.1381,  0.1635,  1.0691, -0.2020,\n",
      "           0.2588],\n",
      "         [ 0.2911,  0.8315, -0.6205, -0.1344,  0.2141,  1.0786, -0.2229,\n",
      "           0.2435],\n",
      "         [ 0.2877,  0.8157, -0.6089, -0.0842,  0.1843,  1.0614, -0.2081,\n",
      "           0.2494]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "=== After Cross-Attention and Norm (Decoder) ===\n",
      "tensor([[[-0.3721,  0.1765, -0.0757,  0.0307, -0.1435,  2.3639, -1.3474,\n",
      "          -0.6324],\n",
      "         [-0.2607,  0.3070,  0.8583, -0.7458, -1.0087,  1.9483, -1.3003,\n",
      "           0.2019],\n",
      "         [ 0.0719, -0.9776,  1.1193, -0.5912, -1.1167,  2.0136, -0.2736,\n",
      "          -0.2456],\n",
      "         [ 0.9029, -0.7415, -0.3022, -0.7185, -1.4791,  1.8513, -0.1396,\n",
      "           0.6267]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Input to FeedForward Network:\n",
      "tensor([[[-0.3721,  0.1765, -0.0757,  0.0307, -0.1435,  2.3639, -1.3474,\n",
      "          -0.6324],\n",
      "         [-0.2607,  0.3070,  0.8583, -0.7458, -1.0087,  1.9483, -1.3003,\n",
      "           0.2019],\n",
      "         [ 0.0719, -0.9776,  1.1193, -0.5912, -1.1167,  2.0136, -0.2736,\n",
      "          -0.2456],\n",
      "         [ 0.9029, -0.7415, -0.3022, -0.7185, -1.4791,  1.8513, -0.1396,\n",
      "           0.6267]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "After first linear layer (fc1):\n",
      "tensor([[[ 0.3081, -1.1641,  0.4622, -0.5259, -0.5887,  0.1110,  0.3457,\n",
      "          -0.2354,  0.1185,  0.0803,  0.7845, -0.7414,  0.7743,  0.3419,\n",
      "           0.0784, -0.9168, -0.7401, -0.0684, -0.0282, -0.1628, -1.2306,\n",
      "          -0.6606, -0.2588, -0.0381, -0.4109,  0.6351,  0.4501, -0.2639,\n",
      "          -0.0385, -0.2986,  0.3212,  0.4097],\n",
      "         [ 0.1763, -0.9243,  0.8799, -0.5313, -0.7759, -0.2869,  0.3696,\n",
      "          -0.6660,  0.5932,  0.3333,  0.9138, -0.5930,  0.6415,  0.1372,\n",
      "          -0.0636, -0.3361, -0.0993, -0.6613, -0.2938,  0.1573, -0.7510,\n",
      "          -0.9444,  0.1040, -0.4756, -0.6236,  0.0884,  0.6674,  0.4433,\n",
      "           0.3780, -0.2997,  0.4860, -0.1730],\n",
      "         [ 0.4212, -1.1265,  0.2939, -0.7023, -0.8946, -0.8069,  0.4079,\n",
      "          -0.5361,  0.9307,  0.0113,  0.4440, -0.9743,  0.4919,  0.3721,\n",
      "           0.0639,  0.0187,  0.1925, -0.8401, -0.6413, -0.6877, -0.8869,\n",
      "          -1.2187,  0.6758,  0.1365,  0.0849,  0.7529,  1.0432,  0.3408,\n",
      "           0.5193, -0.7113,  0.4908,  0.0351],\n",
      "         [-0.3526, -1.2045,  0.4371, -0.5214, -0.8637, -0.6168,  0.5121,\n",
      "           0.0239,  0.3360, -0.3105,  0.3537, -0.1326,  0.2320,  0.9125,\n",
      "           0.5753,  0.0394,  0.2745, -0.5995, -1.2291, -0.4493, -1.1227,\n",
      "          -0.6388,  0.4377, -0.6396,  0.2877,  0.5252,  0.6656,  0.6074,\n",
      "           0.9523, -0.3275,  1.1311, -0.6390]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "After ReLU Activation:\n",
      "tensor([[[0.3081, 0.0000, 0.4622, 0.0000, 0.0000, 0.1110, 0.3457, 0.0000,\n",
      "          0.1185, 0.0803, 0.7845, 0.0000, 0.7743, 0.3419, 0.0784, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.6351, 0.4501, 0.0000, 0.0000, 0.0000, 0.3212, 0.4097],\n",
      "         [0.1763, 0.0000, 0.8799, 0.0000, 0.0000, 0.0000, 0.3696, 0.0000,\n",
      "          0.5932, 0.3333, 0.9138, 0.0000, 0.6415, 0.1372, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.1573, 0.0000, 0.0000, 0.1040, 0.0000,\n",
      "          0.0000, 0.0884, 0.6674, 0.4433, 0.3780, 0.0000, 0.4860, 0.0000],\n",
      "         [0.4212, 0.0000, 0.2939, 0.0000, 0.0000, 0.0000, 0.4079, 0.0000,\n",
      "          0.9307, 0.0113, 0.4440, 0.0000, 0.4919, 0.3721, 0.0639, 0.0187,\n",
      "          0.1925, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6758, 0.1365,\n",
      "          0.0849, 0.7529, 1.0432, 0.3408, 0.5193, 0.0000, 0.4908, 0.0351],\n",
      "         [0.0000, 0.0000, 0.4371, 0.0000, 0.0000, 0.0000, 0.5121, 0.0239,\n",
      "          0.3360, 0.0000, 0.3537, 0.0000, 0.2320, 0.9125, 0.5753, 0.0394,\n",
      "          0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4377, 0.0000,\n",
      "          0.2877, 0.5252, 0.6656, 0.6074, 0.9523, 0.0000, 1.1311, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "\n",
      "After second linear layer (fc2):\n",
      "tensor([[[-0.1443,  0.2762,  0.1292, -0.0376, -0.0972,  0.1509, -0.2289,\n",
      "          -0.0211],\n",
      "         [-0.2420,  0.3267,  0.0421, -0.3151,  0.0527,  0.2122, -0.1648,\n",
      "          -0.2608],\n",
      "         [-0.3420,  0.5424, -0.0773, -0.1805, -0.1113,  0.3077, -0.1014,\n",
      "          -0.2587],\n",
      "         [-0.3233,  0.4830, -0.2699, -0.2657, -0.1370,  0.2624,  0.0516,\n",
      "          -0.1974]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "=== After FeedForward and Norm (Decoder) ===\n",
      "tensor([[[-0.4704,  0.4065,  0.0454, -0.0093, -0.2209,  2.2727, -1.4296,\n",
      "          -0.5944],\n",
      "         [-0.4045,  0.5967,  0.8317, -0.8962, -0.8038,  1.9419, -1.2523,\n",
      "          -0.0135],\n",
      "         [-0.2257, -0.3795,  0.9959, -0.6927, -1.1176,  2.1869, -0.3234,\n",
      "          -0.4438],\n",
      "         [ 0.5948, -0.1975, -0.4940, -0.8837, -1.4810,  2.0452, -0.0364,\n",
      "           0.4526]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "=== Final Decoder Output (logits) ===\n",
      "tensor([[[ 0.0216, -0.3942, -0.0221,  0.4009, -0.6981],\n",
      "         [-0.3733, -0.5489, -0.3494,  0.3489, -0.2700],\n",
      "         [-0.4157, -0.7180, -0.2716,  0.1552, -0.2928],\n",
      "         [ 0.0016, -0.6232,  0.2361,  0.1670, -0.0693]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "=== Final Output Logits from Transformer ===\n",
      "tensor([[[ 0.0216, -0.3942, -0.0221,  0.4009, -0.6981],\n",
      "         [-0.3733, -0.5489, -0.3494,  0.3489, -0.2700],\n",
      "         [-0.4157, -0.7180, -0.2716,  0.1552, -0.2928],\n",
      "         [ 0.0016, -0.6232,  0.2361,  0.1670, -0.0693]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# Utility: Tokenizer and vocabulary creation #\n",
    "##############################################\n",
    "# Source and target example sentences\n",
    "sentence_en = \"I love AI .\"\n",
    "sentence_fr = \"J' adore l'IA .\"\n",
    "\n",
    "# Note: Vocabulary mapping must match the tokens that appear (you can extend these as needed).\n",
    "word_map_en = {\"<pad>\": 0, \"I\": 1, \"love\": 2, \"AI\": 3, \".\": 4}\n",
    "word_map_fr = {\"<pad>\": 0, \"J'\": 1, \"adore\": 2, \"l'IA\": 3, \".\": 4}\n",
    "\n",
    "def tokenize(sentence, word_map):\n",
    "    tokens = [word_map[word] for word in sentence.split()]\n",
    "    print(f\"Tokens for '{sentence}': {tokens}\")\n",
    "    return torch.tensor(tokens)\n",
    "\n",
    "##############################################\n",
    "# Positional Encoding (used by both encoder and decoder)\n",
    "##############################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * \n",
    "                             -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "\n",
    "        # Register as buffer so itâ€™s saved in state_dict but not updated by optimizer.\n",
    "        self.register_buffer(\"pe\", encoding.unsqueeze(0))  # shape (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        pos_enc = self.pe[:, :x.size(1)]\n",
    "        print(\"\\nPositional Encoding Values:\")\n",
    "        print(pos_enc)\n",
    "        x = x + pos_enc\n",
    "        print(\"\\nEmbeddings After Adding Positional Encoding:\")\n",
    "        print(x)\n",
    "        return x\n",
    "\n",
    "##############################################\n",
    "# Multi-Head Attention (supports separate Q/K/V for both encoder and decoder)\n",
    "##############################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "\n",
    "        # Linear transformations for Q, K, V.\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final output linear layer.\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key=None, value=None, mask=None):\n",
    "        # If key/value are not provided, use query (for self-attention)\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1)\n",
    "        seq_len_k = key.size(1)\n",
    "\n",
    "        # Linear projections and reshape for multiple heads\n",
    "        q = self.linear_q(query).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.linear_k(key).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.linear_v(value).view(batch_size, seq_len_k, self.num_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        print(\"\\nQuery Matrix (Q):\")\n",
    "        print(q)\n",
    "        print(\"\\nKey Matrix (K):\")\n",
    "        print(k)\n",
    "        print(\"\\nValue Matrix (V):\")\n",
    "        print(v)\n",
    "\n",
    "        # Compute scaled dot-product attention scores.\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        print(\"\\nRaw Attention Scores (before softmax):\")\n",
    "        print(attn_scores)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask should be broadcastable to [batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        print(\"\\nAttention Weights (after softmax):\")\n",
    "        print(attn_weights)\n",
    "\n",
    "        # Multiply the weights by values.\n",
    "        attention_output = torch.matmul(attn_weights, v)\n",
    "        # Concatenate multiple heads and apply a final linear transform.\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        print(\"\\nAttention Output (before final FC layer):\")\n",
    "        print(attention_output)\n",
    "\n",
    "        output = self.fc(attention_output)\n",
    "        print(\"\\nFinal Output from Multi-Head Attention (after FC):\")\n",
    "        print(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "##############################################\n",
    "# Feed-Forward Network\n",
    "##############################################\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=512):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\nInput to FeedForward Network:\")\n",
    "        print(x)\n",
    "        x_fc1 = self.fc1(x)\n",
    "        print(\"\\nAfter first linear layer (fc1):\")\n",
    "        print(x_fc1)\n",
    "        x_relu = F.relu(x_fc1)\n",
    "        print(\"\\nAfter ReLU Activation:\")\n",
    "        print(x_relu)\n",
    "        x_fc2 = self.fc2(x_relu)\n",
    "        print(\"\\nAfter second linear layer (fc2):\")\n",
    "        print(x_fc2)\n",
    "        return x_fc2\n",
    "\n",
    "##############################################\n",
    "# Encoder Layer (one layer for simplicity)\n",
    "##############################################\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=512):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        print(\"\\n=== Encoder Layer Input ===\")\n",
    "        print(x)\n",
    "        attn_output = self.self_attn(x, mask=mask)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        print(\"\\n=== After Self-Attention and Norm (Encoder) ===\")\n",
    "        print(x)\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        print(\"\\n=== After FeedForward and Norm (Encoder) ===\")\n",
    "        print(x)\n",
    "        return x\n",
    "\n",
    "##############################################\n",
    "# Decoder Layer (with masked self-attention and cross-attention)\n",
    "##############################################\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=512):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
    "        print(\"\\n=== Decoder Layer Input ===\")\n",
    "        print(x)\n",
    "        # 1. Masked self-attention (causal mask prevents future lookahead)\n",
    "        self_attn_out = self.self_attn(x, x, x, mask=self_mask)\n",
    "        x = self.norm1(x + self_attn_out)\n",
    "        print(\"\\n=== After Masked Self-Attention and Norm (Decoder) ===\")\n",
    "        print(x)\n",
    "        # 2. Cross-attention (queries from decoder x, keys/values from encoder_output)\n",
    "        cross_attn_out = self.cross_attn(x, encoder_output, encoder_output, mask=cross_mask)\n",
    "        x = self.norm2(x + cross_attn_out)\n",
    "        print(\"\\n=== After Cross-Attention and Norm (Decoder) ===\")\n",
    "        print(x)\n",
    "        # 3. Feed-Forward Network.\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.norm3(x + ff_output)\n",
    "        print(\"\\n=== After FeedForward and Norm (Decoder) ===\")\n",
    "        print(x)\n",
    "        return x\n",
    "\n",
    "##############################################\n",
    "# Functions to build full encoder and decoder stacks\n",
    "##############################################\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        print(\"\\n=== Encoder Input Tokens ===\")\n",
    "        print(x)\n",
    "        x = self.embedding(x)\n",
    "        print(\"\\n=== Token Embeddings ===\")\n",
    "        print(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        print(\"\\n=== Final Encoder Output ===\")\n",
    "        print(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        # Final linear layer to project the decoder output to vocabulary size (for prediction)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
    "        print(\"\\n=== Decoder Input Tokens ===\")\n",
    "        print(x)\n",
    "        x = self.embedding(x)\n",
    "        print(\"\\n=== Decoder Token Embeddings ===\")\n",
    "        print(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, self_mask, cross_mask)\n",
    "        logits = self.fc_out(x)\n",
    "        print(\"\\n=== Final Decoder Output (logits) ===\")\n",
    "        print(logits)\n",
    "        return logits\n",
    "\n",
    "##############################################\n",
    "# Function to generate a causal mask for decoder self-attention.\n",
    "##############################################\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    # Creates a lower-triangular matrix of ones.\n",
    "    mask = torch.tril(torch.ones(sz, sz)).unsqueeze(0).unsqueeze(0)  # shape: (1, 1, sz, sz)\n",
    "    return mask  # 1s for allowed positions; 0s elsewhere.\n",
    "\n",
    "##############################################\n",
    "# Full Transformer Model combining the Encoder and Decoder\n",
    "##############################################\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=8, num_heads=2, d_ff=32, num_enc_layers=1, num_dec_layers=1, max_len=5000):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, d_ff, num_enc_layers, max_len)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, d_ff, num_dec_layers, max_len)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, cross_mask=None):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        output = self.decoder(tgt, encoder_output, self_mask=tgt_mask, cross_mask=cross_mask)\n",
    "        return output\n",
    "\n",
    "##############################################\n",
    "# Example Usage: Run full Transformer on tokenized sentences\n",
    "##############################################\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "\n",
    "    # Tokenize source and target sentences (shape will be [seq_len])\n",
    "    src_tokens = tokenize(sentence_en, word_map_en)  # e.g., [1, 2, 3, 4]\n",
    "    tgt_tokens = tokenize(sentence_fr, word_map_fr)  # e.g., [1, 2, 3, 4]\n",
    "    \n",
    "    # Add a batch dimension: shape becomes (batch_size, seq_len)\n",
    "    src_tokens = src_tokens.unsqueeze(0)\n",
    "    tgt_tokens = tgt_tokens.unsqueeze(0)\n",
    "\n",
    "    # Model hyperparameters (d_model=8, vocab sizes per your word maps)\n",
    "    d_model = 8\n",
    "    num_heads = 2\n",
    "    d_ff = 32  # Setting a small feedforward inner layer for clarity\n",
    "    num_enc_layers = 1\n",
    "    num_dec_layers = 1\n",
    "\n",
    "    transformer_model = Transformer(src_vocab_size=len(word_map_en),\n",
    "                                    tgt_vocab_size=len(word_map_fr),\n",
    "                                    d_model=d_model,\n",
    "                                    num_heads=num_heads,\n",
    "                                    d_ff=d_ff,\n",
    "                                    num_enc_layers=num_enc_layers,\n",
    "                                    num_dec_layers=num_dec_layers)\n",
    "\n",
    "    # Create mask for target (decoder) self-attention so that each token can only attend to previous tokens.\n",
    "    tgt_seq_len = tgt_tokens.size(1)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "\n",
    "    # For simplicity, we are not applying any special mask in encoder or cross-attention.\n",
    "    src_mask = None\n",
    "    cross_mask = None\n",
    "\n",
    "    # Run the full Transformer: it will print intermediate values along the way.\n",
    "    output_logits = transformer_model(src_tokens, tgt_tokens, src_mask=src_mask, tgt_mask=tgt_mask, cross_mask=cross_mask)\n",
    "\n",
    "    print(\"\\n=== Final Output Logits from Transformer ===\")\n",
    "    print(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd73255-5241-4149-8881-f9c8ea7662f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  8 07:25:48 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   47C    P8               9W / 320W |   6040MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790fa74-4b52-4ad4-901c-b32726c6dc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
