{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWOaAM08geJV"
   },
   "source": [
    "# Building a Sentiment Analysis Model with LSTM on a Small Text Dataset\n",
    "The goal is to build a sentiment analysis model using a Long Short-Term Memory (LSTM) network to classify the sentiment of text data (e.g., reviews, tweets, or comments) as positive, negative. The model will learn to capture sequential dependencies in text and make accurate predictions based on the context and tone of the input."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1: Import Necessary Libraries\n",
    "2: Data Preprocessing\n",
    " \t1:Loading the Dataset\n",
    "\t2:Tokenization\n",
    "\t3:Train Word2Vec on tokenized sentences\n",
    "\t4:Convert sentences into fixed-length vectors\n",
    "\t5:Converting numpy arrays to tensors\n",
    "3.LSTM Model Building\t\n",
    "\t1:Defining the LSTM Model\n",
    "\t2:Initialize the model parameters\n",
    "\t3:Initialize the model,loss and optimizer\n",
    "\t4:Training the Model\n",
    "4.Model Evaluation\n",
    "5:Sample Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pandas: Import this library to work with data in tables (DataFrames).\n",
    "NumPy: Import this library to perform mathematical operations on numbers and arrays.\n",
    "Regular Expressions: Import this library to help with text processing (like removing unwanted characters).\n",
    "Word2Vec: Import this tool from Gensim to create word embeddings from text.\n",
    "PyTorch: Import necessary parts of this library to build and train your LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JH7pkOSQgXaI",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8ad9aab5e45bc886",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your task is to create a small dataset of text reviews and sentiments. An example dataset is shown below:\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"review\": [\n",
    "        # Positive reviews\n",
    "        \"The movie was absolutely amazing! A masterpiece.\",\n",
    "        \"Fantastic performances by the entire cast.\",\n",
    "        \"Loved the story and the cinematography. A must-watch!\",\n",
    "        \"The action scenes were thrilling and well-executed.\",\n",
    "        \"An emotional and heartwarming experience.\",\n",
    "        \"Brilliant direction and an outstanding soundtrack.\",\n",
    "        \"A gripping storyline that kept me on the edge of my seat.\",\n",
    "        \"One of the best movies I've ever seen!\",\n",
    "        \"The characters were relatable and well-developed.\",\n",
    "        \"A perfect blend of humor and drama.\",\n",
    "        \n",
    "        # Negative reviews\n",
    "        \"The movie was a complete waste of time.\",\n",
    "        \"Terrible acting and a poorly written script.\",\n",
    "        \"The story was boring and predictable.\",\n",
    "        \"I couldn’t relate to any of the characters.\",\n",
    "        \"The pacing was slow and the plot lacked depth.\",\n",
    "        \"Way too many plot holes, very disappointing.\",\n",
    "        \"The humor felt forced and awkward.\",\n",
    "        \"It was overhyped and did not live up to expectations.\",\n",
    "        \"The ending was rushed and unsatisfying.\",\n",
    "        \"Poor direction and lackluster performances.\"\n",
    "    ],\n",
    "    \"sentiment\": [\n",
    "        # Sentiments for positive reviews\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        # Sentiments for negative reviews\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    ]  # 1 = Positive, 0 = Negative\n",
    "}\n",
    "\n",
    "Convert the dataset into a Pandas DataFrame and store in a variable named df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9qxdcSAghlv"
   },
   "source": [
    "## Step1:Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8rl_upg0glJM",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-77fe92777223f025",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review  sentiment\n",
      "0    The movie was absolutely amazing! A masterpiece.          1\n",
      "1          Fantastic performances by the entire cast.          1\n",
      "2   Loved the story and the cinematography. A must...          1\n",
      "3   The action scenes were thrilling and well-exec...          1\n",
      "4           An emotional and heartwarming experience.          1\n",
      "5   Brilliant direction and an outstanding soundtr...          1\n",
      "6   A gripping storyline that kept me on the edge ...          1\n",
      "7              One of the best movies I've ever seen!          1\n",
      "8   The characters were relatable and well-developed.          1\n",
      "9                 A perfect blend of humor and drama.          1\n",
      "10            The movie was a complete waste of time.          0\n",
      "11       Terrible acting and a poorly written script.          0\n",
      "12              The story was boring and predictable.          0\n",
      "13        I couldn’t relate to any of the characters.          0\n",
      "14     The pacing was slow and the plot lacked depth.          0\n",
      "15       Way too many plot holes, very disappointing.          0\n",
      "16                 The humor felt forced and awkward.          0\n",
      "17  It was overhyped and did not live up to expect...          0\n",
      "18            The ending was rushed and unsatisfying.          0\n",
      "19        Poor direction and lackluster performances.          0\n"
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Balanced dataset with positive and negative reviews\n",
    "data = {\n",
    "    \"review\": [\n",
    "        # Positive reviews\n",
    "        \"The movie was absolutely amazing! A masterpiece.\",\n",
    "        \"Fantastic performances by the entire cast.\",\n",
    "        \"Loved the story and the cinematography. A must-watch!\",\n",
    "        \"The action scenes were thrilling and well-executed.\",\n",
    "        \"An emotional and heartwarming experience.\",\n",
    "        \"Brilliant direction and an outstanding soundtrack.\",\n",
    "        \"A gripping storyline that kept me on the edge of my seat.\",\n",
    "        \"One of the best movies I've ever seen!\",\n",
    "        \"The characters were relatable and well-developed.\",\n",
    "        \"A perfect blend of humor and drama.\",\n",
    "        \n",
    "        # Negative reviews\n",
    "        \"The movie was a complete waste of time.\",\n",
    "        \"Terrible acting and a poorly written script.\",\n",
    "        \"The story was boring and predictable.\",\n",
    "        \"I couldn’t relate to any of the characters.\",\n",
    "        \"The pacing was slow and the plot lacked depth.\",\n",
    "        \"Way too many plot holes, very disappointing.\",\n",
    "        \"The humor felt forced and awkward.\",\n",
    "        \"It was overhyped and did not live up to expectations.\",\n",
    "        \"The ending was rushed and unsatisfying.\",\n",
    "        \"Poor direction and lackluster performances.\"\n",
    "    ],\n",
    "    \"sentiment\": [\n",
    "        # Sentiments for positive reviews\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        # Sentiments for negative reviews\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    ]  # 1 = Positive, 0 = Negative\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt3icKmVgq4j"
   },
   "source": [
    "## Step2:Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Define a Tokenization Function:\n",
    "\n",
    "    Write a Python function named tokenize that takes a single input:\n",
    "    text: A string representing a movie review.\n",
    "    The function should:\n",
    "    Convert the text to lowercase.\n",
    "    Use regular expressions (re) to extract all words (alphanumeric sequences).\n",
    "2. Apply the Function to a Dataset\n",
    "    Convert the dataset into a Pandas DataFrame.\n",
    "3.Tokenize Each Review:\n",
    "    Use the tokenize function to process each review in the review column of the DataFrame.\n",
    "    Store the resulting tokens in a new column named tokens.i,e, df['tokens']\n",
    "4.Verify Results:\n",
    "    Display the DataFrame with the tokens column to confirm that the tokenization process was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_5fEZCM9gqif",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-676e554e00aaa4a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [the, movie, was, absolutely, amazing, a, mast...\n",
       "1      [fantastic, performances, by, the, entire, cast]\n",
       "2     [loved, the, story, and, the, cinematography, ...\n",
       "3     [the, action, scenes, were, thrilling, and, we...\n",
       "4        [an, emotional, and, heartwarming, experience]\n",
       "5     [brilliant, direction, and, an, outstanding, s...\n",
       "6     [a, gripping, storyline, that, kept, me, on, t...\n",
       "7       [one, of, the, best, movies, i, ve, ever, seen]\n",
       "8     [the, characters, were, relatable, and, well, ...\n",
       "9            [a, perfect, blend, of, humor, and, drama]\n",
       "10      [the, movie, was, a, complete, waste, of, time]\n",
       "11    [terrible, acting, and, a, poorly, written, sc...\n",
       "12          [the, story, was, boring, and, predictable]\n",
       "13    [i, couldn, t, relate, to, any, of, the, chara...\n",
       "14    [the, pacing, was, slow, and, the, plot, lacke...\n",
       "15    [way, too, many, plot, holes, very, disappoint...\n",
       "16             [the, humor, felt, forced, and, awkward]\n",
       "17    [it, was, overhyped, and, did, not, live, up, ...\n",
       "18        [the, ending, was, rushed, and, unsatisfying]\n",
       "19     [poor, direction, and, lackluster, performances]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize function using regex\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Apply the tokenization function\n",
    "df['tokens'] = df['review'].apply(tokenize)\n",
    "df['tokens']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3MOU81ag3en"
   },
   "source": [
    "## Step 3:Train Word2Vec on tokenized sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Input Data:\n",
    "    You have a DataFrame df containing a column tokens. This column consists of tokenized movie reviews (lists of words).\n",
    "2.Use the Word2Vec Class:\n",
    "    Import the Word2Vec class from the gensim.models module.\n",
    "    Train a Word2Vec model on the tokens column of the DataFrame.\n",
    "3.Key Parameters for Word2Vec:\n",
    "    sentences: The input data for training. Use df['tokens'] as the list of tokenized sentences.\n",
    "    vector_size: The dimensionality of the word vectors (e.g., 50).\n",
    "    window: The maximum distance between the target word and its context words (e.g., 3).\n",
    "    min_count: The minimum frequency a word must have to be included in the vocabulary (e.g., 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8DVtcqn1gz4H",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6573b9e7fc5fd28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=97, vector_size=50, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec on tokenized sentences\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=50, window=3, min_count=1)\n",
    "print(word2vec_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxcCH6PXhBIA"
   },
   "source": [
    "## Step 4:Convert sentences into fixed-length vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Write a function sentence_to_vectors(tokens, model, max_len) that:\n",
    "\n",
    "    Takes a tokenized sentence (tokens), a trained Word2Vec model (model), and a maximum sentence length (max_len).\n",
    "    Converts each word in the sentence into its corresponding vector using the Word2Vec model.\n",
    "    Pads the vector list with zeros or truncates it to ensure a fixed length of max_len.\n",
    "    Returns the resulting vector representation as a NumPy array.\n",
    "2. Convert all sentences in df['tokens'] into fixed-length vectors using your function. Store the resulting vectors in X.\n",
    "3. Convert the sentiment labels from df['sentiment'] into a NumPy array and store them in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EpAywrtThCEI",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06f1c7b61b055104",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert sentences into fixed-length vectors\n",
    "def sentence_to_vectors(tokens, model, max_len=5):\n",
    "    \n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in model.wv:  # Check if the word is in the model\n",
    "            vectors.append(model.wv[word])  # Append the word's vector\n",
    "    # Pad or truncate to max_len\n",
    "    vectors = vectors[:max_len] + [[0] * model.vector_size] * (max_len - len(vectors))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Convert reviews to vectors\n",
    "max_len = 5  # Fixed input length\n",
    "X = np.array([sentence_to_vectors(tokens, word2vec_model, max_len) for tokens in df['tokens']])\n",
    "y = np.array(df['sentiment'])\n",
    "#df['vectors'] = df['tokens'].apply(lambda tokens: sentence_to_vectors(tokens, word2vec_model, max_len))\n",
    "\n",
    "# Display the DataFrame with vectors\n",
    "#print(df[['review', 'tokens', 'vectors']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Converting numpy arrays to tensors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If there are n sentences, the final shape of X will be (n, max_len, vector_size).\n",
    "\n",
    "You have already created:\n",
    "X: A 3D NumPy array containing vectorized representations of sentences.\n",
    "y: A 1D NumPy array containing the sentiment labels corresponding to each sentence.\n",
    "\n",
    "Objective:\n",
    "\n",
    "Convert the following into PyTorch tensors:\n",
    "X into a tensor of data type torch.float32 (used for real-valued inputs).\n",
    "y into a tensor of data type torch.long (used for classification labels).\n",
    "\n",
    "Steps to Follow:\n",
    "Use the torch.tensor() function to convert both X and y into PyTorch tensors.\n",
    "Ensure that X is of data type torch.float32 and y is of data type torch.long.\n",
    "Store the resulting tensors as X_tensor and y_tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc198d1113de36fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0011,  0.0005,  0.0102,  ...,  0.0192,  0.0100,  0.0185],\n",
      "         [ 0.0028, -0.0052, -0.0142,  ...,  0.0010,  0.0164, -0.0141],\n",
      "         [ 0.0156, -0.0190, -0.0004,  ..., -0.0048, -0.0190,  0.0090],\n",
      "         [ 0.0036,  0.0141,  0.0059,  ..., -0.0067,  0.0032,  0.0032],\n",
      "         [ 0.0148,  0.0200,  0.0177,  ..., -0.0037,  0.0072, -0.0141]],\n",
      "\n",
      "        [[-0.0104, -0.0148, -0.0058,  ..., -0.0054,  0.0077,  0.0007],\n",
      "         [ 0.0163, -0.0089,  0.0180,  ..., -0.0059,  0.0183,  0.0017],\n",
      "         [ 0.0113,  0.0110,  0.0037,  ..., -0.0176,  0.0069,  0.0042],\n",
      "         [-0.0011,  0.0005,  0.0102,  ...,  0.0192,  0.0100,  0.0185],\n",
      "         [-0.0188, -0.0099, -0.0194,  ...,  0.0146,  0.0109,  0.0185]],\n",
      "\n",
      "        [[ 0.0122, -0.0135,  0.0014,  ..., -0.0033, -0.0189, -0.0052],\n",
      "         [-0.0011,  0.0005,  0.0102,  ...,  0.0192,  0.0100,  0.0185],\n",
      "         [-0.0165,  0.0186, -0.0004,  ..., -0.0048, -0.0063, -0.0047],\n",
      "         [-0.0163,  0.0091, -0.0083,  ..., -0.0141,  0.0018,  0.0128],\n",
      "         [-0.0011,  0.0005,  0.0102,  ...,  0.0192,  0.0100,  0.0185]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0044, -0.0194,  0.0186,  ...,  0.0075, -0.0013, -0.0197],\n",
      "         [ 0.0156, -0.0190, -0.0004,  ..., -0.0048, -0.0190,  0.0090],\n",
      "         [ 0.0128, -0.0179, -0.0147,  ..., -0.0017,  0.0004,  0.0176],\n",
      "         [-0.0163,  0.0091, -0.0083,  ..., -0.0141,  0.0018,  0.0128],\n",
      "         [-0.0072, -0.0138,  0.0015,  ...,  0.0043,  0.0190, -0.0117]],\n",
      "\n",
      "        [[-0.0011,  0.0005,  0.0102,  ...,  0.0192,  0.0100,  0.0185],\n",
      "         [ 0.0159, -0.0129,  0.0116,  ..., -0.0027,  0.0197,  0.0006],\n",
      "         [ 0.0156, -0.0190, -0.0004,  ..., -0.0048, -0.0190,  0.0090],\n",
      "         [ 0.0100,  0.0181, -0.0195,  ...,  0.0166, -0.0072, -0.0197],\n",
      "         [-0.0163,  0.0091, -0.0083,  ..., -0.0141,  0.0018,  0.0128]],\n",
      "\n",
      "        [[ 0.0115,  0.0057,  0.0136,  ..., -0.0184, -0.0103,  0.0155],\n",
      "         [-0.0006, -0.0177, -0.0172,  ..., -0.0115, -0.0033,  0.0111],\n",
      "         [-0.0163,  0.0091, -0.0083,  ..., -0.0141,  0.0018,  0.0128],\n",
      "         [ 0.0193,  0.0147,  0.0025,  ..., -0.0144, -0.0034, -0.0082],\n",
      "         [ 0.0163, -0.0089,  0.0180,  ..., -0.0059,  0.0183,  0.0017]]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "print(X_tensor)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.LSTM Model Building"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model Requirements\n",
    "Input Layer:\n",
    "Accepts a sequence of word embeddings with dimensions (batch_size, sequence_length, input_size).\n",
    "\n",
    "LSTM Layer:\n",
    "Use PyTorch's nn.LSTM to process the input sequence and learn temporal relationships.\n",
    "Configure the LSTM with:\n",
    "    input_size: Size of the word vectors (e.g., 50).\n",
    "    hidden_size: Size of the LSTM's hidden state (e.g., 32).\n",
    "    batch_first=True to ensure input tensors have the shape (batch_size, sequence_length, input_size).\n",
    "Fully Connected Layer:\n",
    "    A linear layer that maps the LSTM's hidden state to the output size (number of classes).\n",
    "    The number of output classes should be passed as a parameter (output_size).\n",
    "\n",
    "Forward Pass:\n",
    "Pass the input sequence through the LSTM.\n",
    "Use the last hidden state of the LSTM to generate the final output.\n",
    "Pass this hidden state through the fully connected layer to obtain class predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOAVMHYGhGAe"
   },
   "source": [
    "## Step 1:Defining the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z-36YsXphI9d",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e618061d60c964de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)  # Get the hidden state from LSTM\n",
    "        out = self.fc(hidden[-1])     # Pass hidden state through a fully connected layer\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2:Initialize the model parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Define the input_size based on the dimensions of your word vectors.(50)\n",
    "Choose a reasonable hidden_size based on the complexity of the problem.(32) \n",
    "Larger values can capture more patterns but may lead to overfitting.\n",
    "Set the output_size to 2, as this is a binary classification task.\n",
    "Ensure you have already defined the SimpleLSTM class before initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-409e0cb70fffedf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = 50  # Size of the word vector\n",
    "hidden_size = 32\n",
    "output_size = 2  # Binary classification (positive/negative)as we  are working with one-hot encoded labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3:Initialize the model,loss and optimizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Define the Loss Function:\n",
    "\n",
    "Use the CrossEntropyLoss provided by PyTorch. It is suitable for multi-class classification tasks, including binary classification and store it in a variable \"criterion\"\n",
    "Choose an Optimizer:\n",
    "\n",
    "Use the Adam optimizer (torch.optim.Adam), which adapts learning rates for each parameter and is well-suited for neural networks.store it in variable \"optimizer\"\n",
    "    Set the learning rate to 0.01.\n",
    "    Link the Optimizer to the Model:\n",
    "\n",
    "Pass the model's parameters (model.parameters()) to the optimizer so it knows which parameters to update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84fb6f1d37a21f9e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP2OlEfohPbT"
   },
   "source": [
    "## Step 4:Training the Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You need the implement the training loop by following the below instructions\n",
    "Set the Number of Epochs:\n",
    "Define the number of epochs (iterations over the entire training dataset) for which the model will be trained. In this case, set it to 10.\n",
    "\n",
    "Training Loop:\n",
    "For each epoch:\n",
    "    Set the model to training mode using model.train().\n",
    "    Reset the gradients of the optimizer with optimizer.zero_grad() to prevent accumulation of gradients from previous iterations.\n",
    "\n",
    "    Forward Pass:\n",
    "    Pass the training data (X_tensor) through the model to obtain predictions (outputs).\n",
    "\n",
    "    Calculate Loss:\n",
    "    Use the defined loss function (criterion) to compute the loss between the model's predictions (outputs) and the true labels (y_tensor).\n",
    "\n",
    "    Backward Pass and Optimization:\n",
    "    Call loss.backward() to compute the gradients.\n",
    "    Use optimizer.step() to update the model parameters based on the computed gradients.\n",
    "\n",
    "    Print Loss:\n",
    "    After each epoch, print the current epoch number and the loss value for monitoring the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4b5LBMXhQXG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b863baceaf2e7910",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "f1e82052-30d9-4fc0-d663-78eb58b1a0c1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6939\n",
      "Epoch 2/50, Loss: 0.6931\n",
      "Epoch 3/50, Loss: 0.6920\n",
      "Epoch 4/50, Loss: 0.6909\n",
      "Epoch 5/50, Loss: 0.6901\n",
      "Epoch 6/50, Loss: 0.6892\n",
      "Epoch 7/50, Loss: 0.6877\n",
      "Epoch 8/50, Loss: 0.6860\n",
      "Epoch 9/50, Loss: 0.6840\n",
      "Epoch 10/50, Loss: 0.6815\n",
      "Epoch 11/50, Loss: 0.6781\n",
      "Epoch 12/50, Loss: 0.6738\n",
      "Epoch 13/50, Loss: 0.6684\n",
      "Epoch 14/50, Loss: 0.6607\n",
      "Epoch 15/50, Loss: 0.6512\n",
      "Epoch 16/50, Loss: 0.6377\n",
      "Epoch 17/50, Loss: 0.6213\n",
      "Epoch 18/50, Loss: 0.5988\n",
      "Epoch 19/50, Loss: 0.5695\n",
      "Epoch 20/50, Loss: 0.5329\n",
      "Epoch 21/50, Loss: 0.5153\n",
      "Epoch 22/50, Loss: 0.5274\n",
      "Epoch 23/50, Loss: 0.4242\n",
      "Epoch 24/50, Loss: 0.4788\n",
      "Epoch 25/50, Loss: 0.3620\n",
      "Epoch 26/50, Loss: 0.4155\n",
      "Epoch 27/50, Loss: 0.2993\n",
      "Epoch 28/50, Loss: 0.3544\n",
      "Epoch 29/50, Loss: 0.2409\n",
      "Epoch 30/50, Loss: 0.2973\n",
      "Epoch 31/50, Loss: 0.1879\n",
      "Epoch 32/50, Loss: 0.2361\n",
      "Epoch 33/50, Loss: 0.1436\n",
      "Epoch 34/50, Loss: 0.1892\n",
      "Epoch 35/50, Loss: 0.1033\n",
      "Epoch 36/50, Loss: 0.1367\n",
      "Epoch 37/50, Loss: 0.0801\n",
      "Epoch 38/50, Loss: 0.1070\n",
      "Epoch 39/50, Loss: 0.0606\n",
      "Epoch 40/50, Loss: 0.0800\n",
      "Epoch 41/50, Loss: 0.0528\n",
      "Epoch 42/50, Loss: 0.0643\n",
      "Epoch 43/50, Loss: 0.0459\n",
      "Epoch 44/50, Loss: 0.0525\n",
      "Epoch 45/50, Loss: 0.0392\n",
      "Epoch 46/50, Loss: 0.0446\n",
      "Epoch 47/50, Loss: 0.0333\n",
      "Epoch 48/50, Loss: 0.0387\n",
      "Epoch 49/50, Loss: 0.0270\n",
      "Epoch 50/50, Loss: 0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NVSHARE][WARN]: Couldn't open file /var/run/secrets/kubernetes.io/serviceaccount/namespace to read Pod namespace\n",
      "[NVSHARE][INFO]: Successfully initialized nvshare GPU\n",
      "[NVSHARE][INFO]: Client ID = 90c91f0715813fef\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, y_tensor)  \n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model Evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You need to implement the testing phase of the Sentiment Analysis model after training. The steps should be:\n",
    "\n",
    "Tokenization:\n",
    "The first step is to tokenize the input review. Use the tokenize function you defined earlier to break down the review into individual words.\n",
    "\n",
    "Convert Tokens to Vectors:\n",
    "Use the sentence_to_vectors function to convert the list of tokens into fixed-length vectors using the Word2Vec model.\n",
    "\n",
    "Convert to PyTorch Tensor:\n",
    "Transform the vectors into a PyTorch tensor and add a batch dimension using unsqueeze(0). This is necessary because the model expects input in the shape of (batch_size, sequence_length, input_size).\n",
    "Set Model to Evaluation Mode:\n",
    "\n",
    "Before making predictions, set the model to evaluation mode with model.eval(). This will disable certain layers like dropout that are only used during training.\n",
    "Make Prediction:\n",
    "\n",
    "Use torch.no_grad() to avoid tracking gradients during inference. Pass the input tensor through the model to obtain the output predictions.\n",
    "Extract the predicted sentiment class by using torch.argmax() to get the index of the maximum value in the output tensor.\n",
    "Return the Prediction:\n",
    "\n",
    "Return the predicted sentiment label (0 for negative, 1 for positive) as an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJ22WDiPha26"
   },
   "source": [
    "## Step 1:Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nLwpTnvShYO6",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5dd60931b5588541",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_sentiment(review, model, word2vec_model, max_len=5):\n",
    "    # Tokenize the input review\n",
    "    tokens = tokenize(review)\n",
    "\n",
    "    # Convert tokens to vectors\n",
    "    vectors = sentence_to_vectors(tokens, word2vec_model, max_len)\n",
    "\n",
    "    # Convert to PyTorch tensor and add batch dimension\n",
    "    input_tensor = torch.tensor(vectors, dtype=torch.float32).unsqueeze(0)  # Shape: (1, max_len, input_size)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted = torch.argmax(output, dim=1)  # Get the index of the maximum value (class)\n",
    "\n",
    "    return predicted.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o70Gs7P1hXxc"
   },
   "source": [
    "# 5:Sample Predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Define the list of sample reviews (sample_reviews).\n",
    "Use the predict_sentiment function along with a pre-trained model and word2vec_model to predict the sentiment for each review.\n",
    "Map the predictions to Positive or Negative labels.\n",
    "Print the results in the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hRX1xrcheWH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-859d3be05e6286e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "c9e7d552-d22b-4eb3-de43-edb1a32454f6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \"This movie was great and very entertaining!\" => Sentiment: Negative\n",
      "Review: \"waste\" => Sentiment: Negative\n",
      "Review: \"it was boring.\" => Sentiment: Negative\n",
      "Review: \"one of the best movies.\" => Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Sample reviews for prediction\n",
    "sample_reviews = [\n",
    "    \"This movie was great and very entertaining!\",\n",
    "    \"waste\",\n",
    "    \"it was boring.\",\n",
    "    \"one of the best movies.\"\n",
    "]\n",
    "\n",
    "# Predict and print sentiments for sample reviews\n",
    "for review in sample_reviews:\n",
    "    sentiment = predict_sentiment(review, model, word2vec_model)\n",
    "    sentiment_label = \"Positive\" if sentiment == 1 else \"Negative\"\n",
    "    print(f\"Review: \\\"{review}\\\" => Sentiment: {sentiment_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
